# Temporary Kaggle config with GLCA disabled for testing
model:
  backbone: "deit_tiny_patch16_224"
  image_size: 224
  pretrained: true
  num_classes: 0  # Feature extraction mode
  
  # DCAL specific (Phase 2) - GLCA disabled for testing
  use_glca: false
  use_pwca: true
  glca_blocks: 1      # M=1 (last layer only)
  pwca_blocks: 12     # T=12 (all layers)
  local_query_ratio: 0.3  # R=30%

data:
  dataset_root: "/kaggle/input/nd-twin/ND_TWIN_Dataset_224/ND_TWIN_Dataset_224"
  train_info: "data/train_dataset_infor.json"
  train_pairs: "data/train_twin_pairs.json"
  test_info: "data/test_dataset_infor.json"
  test_pairs: "data/test_twin_pairs.json"
  
  # Data augmentation
  augmentation:
    horizontal_flip: 0.5
    rotation: 15
    color_jitter:
      brightness: 0.2
      contrast: 0.2
      saturation: 0.2
      hue: 0.1
    gaussian_blur: 0.1
    random_erasing: 0.25
  
  # Image preprocessing
  image_size: 224
  mean: [0.485, 0.456, 0.406]
  std: [0.229, 0.224, 0.225]
  
  # Data loading - Kaggle optimized
  batch_size: 16
  num_workers: 2
  pin_memory: true

training:
  # Optimizer
  optimizer: "adam"
  learning_rate: 0.0005
  weight_decay: 0.05
  
  # Scheduler
  scheduler: "cosine"
  warmup_epochs: 3  # Reduced for Kaggle
  
  # Training parameters - Kaggle optimized
  epochs: 50  # Reduced for 12h limit
  accumulation_steps: 1
  gradient_clip: 1.0
  
  # Loss
  loss_type: "triplet"
  triplet_margin: 0.3
  hard_mining: true
  
  # Dynamic loss weights
  use_dynamic_weights: true
  
  # Checkpointing - Save frequently for Kaggle
  save_every: 1
  keep_last: 3  # Keep fewer checkpoints for Kaggle

evaluation:
  metrics:
    - "eer"
    - "tar_at_far"
  far_thresholds: [0.001, 0.01, 0.1]
  
  # Test-time augmentation (Phase 3)
  use_tta: false
  tta_crops: 5

tracking:
  # Experiment tracking - Choose one: 'wandb', 'mlflow', 'none'
  backend: "wandb"
  project_name: "twin_dcal_kaggle"
  experiment_name: "baseline_kaggle"
  
  # WandB specific
  wandb:
    entity: null  # Will use default
    tags: ["kaggle", "baseline", "deit_tiny"]
    notes: "Baseline DCAL training on Kaggle"

# Advanced features (Phase 3)
advanced:
  # Model ensemble
  ensemble:
    enabled: false
    models: ["deit_tiny_patch16_224", "deit_small_patch16_224"]
    weights: [0.6, 0.4]
  
  # Hyperparameter optimization
  hpo:
    enabled: false
    backend: "optuna"
    n_trials: 50
    study_name: "dcal_hpo"
  
  # Profiling
  profiling:
    enabled: false
    profile_memory: true
    profile_schedule: "wait=1,warmup=1,active=3,repeat=2"
